# -*- coding: utf-8 -*-
"""talegenerator_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Z_OllxVEAE9-tM2TbVlm-IK3akjB78W
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install fastai==2.2.5
!pip install transformers

import torch
import transformers
from transformers import AutoModelWithLMHead, PreTrainedTokenizerFast
from fastai.text.all import *
import fastai
import re

print(torch.__version__)
print(transformers.__version__)
print( fastai.__version__)

#download model and tokenizer
tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
  bos_token='</s>', eos_token='</s>', unk_token='<unk>',
  pad_token='<pad>', mask_token='<mask>') 
model = AutoModelWithLMHead.from_pretrained("skt/kogpt2-base-v2")

#test tokenizer
print(tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o"))  #ë¬¸ì¥ì„ í† í¬ë‚˜ì´ì €ë¡œ ë‚˜ëˆˆë‹¤

#test model ouput
text = """ì˜›ë‚  ì–´ëŠ ë§ˆì„ì— í† ë¼ì™€ ê±°ë¶ì´ê°€ ì‚´ê³  ìˆì—ˆìŠµë‹ˆë‹¤. í† ë¼ì™€ ê±°ë¶ì´ëŠ” ë‘˜ë„ ì—†ëŠ” ì¹œêµ¬ì˜€ì–´ìš”. """
input_ids = tokenizer.encode(text)
gen_ids = model.generate(torch.tensor([input_ids]),
                           max_length=128,
                           repetition_penalty=2.0,
                           pad_token_id=tokenizer.pad_token_id,
                           eos_token_id=tokenizer.eos_token_id,
                           bos_token_id=tokenizer.bos_token_id,
                           use_cache=True,                  
                        )
generated = tokenizer.decode(gen_ids[0,:].tolist())
print(generated)

# í•™ìŠµ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
with open('/content/tale_all.txt') as f:
    lines = f.read()
lines=" ".join(lines.split())

lines=re.sub('"', ' ', lines)
lines=re.sub("'", ' ', lines)

#model input output tokenizer
class TransformersTokenizer(Transform):
    def __init__(self, tokenizer): self.tokenizer = tokenizer
    def encodes(self, x): 
        toks = self.tokenizer.tokenize(x)
        return tensor(self.tokenizer.convert_tokens_to_ids(toks))
    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))

#split data
train=lines[:int(len(lines)*0.9)]
test=lines[int(len(lines)*0.9):]
splits = [[0],[1]]

#init dataloader
tls = TfmdLists([train,test], TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)
batch,seq_len = 8,256
dls = tls.dataloaders(bs=batch, seq_len=seq_len)
dls.show_batch(max_n=2)

#gpt2 ouput is tuple, we need just one val
class DropOutput(Callback):
    def after_pred(self): self.learn.pred = self.pred[0]
        
        
learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity()).to_fp16()
lr=learn.lr_find()
print(lr)

learn.fit_one_cycle(5,1e-4)  # 1e-4 ëŠ” 10^-4

# ë¬¸ì¥ ìƒì„±
prompt=""" 
ì˜›ë‚  ì˜›ë‚  í•œ ë§ˆì„ì— ë¯¼ìˆ˜ê°€ ì‚´ì•˜ì–´ìš”. ë¯¼ìˆ˜ëŠ” í•­ìƒ ë‚¨ì„ ì˜ ë„ì™€ì£¼ì—ˆì–´ìš”.
"""
prompt_ids = tokenizer.encode(prompt)
inp = tensor(prompt_ids)[None].cuda()
preds = learn.model.generate(inp,
                           max_length=128,
                           pad_token_id=tokenizer.pad_token_id,
                           eos_token_id=tokenizer.eos_token_id,
                           bos_token_id=tokenizer.bos_token_id,
                           repetition_penalty=2.0,       
                           use_cache=True
                          ) 
sentence = tokenizer.decode(preds[0].cpu().numpy())
print(sentence)

# ëª¨ë¸ ì €ì¥
torch.save(learn.model, "drive/My Drive/Colab Notebooks/content/models/model2_3.pt") # or save it's state_dict, better option

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = torch.load("drive/My Drive/Colab Notebooks/content/models/model2_3.pt")